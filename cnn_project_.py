# -*- coding: utf-8 -*-
"""CNN_Project .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17KuEioU4nEBkX09GNdA6NoCDZc742mQT

# German Traffic Signs Recognition using CNN

German Traffic Sign Recognition using CNN involves training a CNN model with a large dataset of images of German traffic signs, along with the corresponding labels indicating the type of sign present in each image. Once trained, the model can be used to classify new images of German traffic signs by making a prediction based on the features learned during training.

##About the Data :
*   Provided by the Real-time Computer Vision research group 
*   Shows images for different German traffic signs
*   Single-image multi-class classification problem 
*   More than 40 classes 
*   More than 50,000 images 
*   Large,lifelike database

The size of data is large to uploade from a local machine or google drive so  
To fetch the data from kaggle directly using kaggle API :

1.    you need to have an account on kaggle 
2.    Do some steps on your kaggle account, Please check out this link:
https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/ 
3. After uploade (kaggle.json) file to the notebook
4. Just, run the cell below (#Fetching the data) and congrats you have the data to start the party👏😎

#Fetching Data
"""

! pip install kaggle
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download meowmeowmeowmeowmeow/gtsrb-german-traffic-sign
! unzip gtsrb-german-traffic-sign

#Library that we need
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.optimizers import Adam
from PIL import Image
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# reading 
import pandas as pd
Train_images = pd.read_csv('/content/Train.csv')
Test_images = pd.read_csv('/content/Test.csv')
Meta_images = pd.read_csv('/content/Meta.csv')

Train_images.head()

#Roi.X1 : Upper left X coordinate of sign on image
#Roi.y1 : Upper left Y coordinate of sign on image
#Roi.X2 : Lower right X coordinate of sign on image
#Roi.y2 : Lower right Y coordinate of sign on image

# Useful in case to draw a rectangle around the signs

Meta_images.head()

Train_images.info()

dataset_directory = '/content'

import cv2
c = 0
for i in Train_images["ClassId"].sample(frac=1).reset_index(drop=True):  
  c += 1
  image1 = cv2.imread(Train_images["Path"][i])
  image_shape = image1.shape
  print(image_shape)
  if c==10:
    break

# Show pictures of traffic sign and it's labels
plt.figure(figsize=(10, 10))
num = 0
for i in (Meta_images.sort_values(by=['ClassId']))['Path']:
    plt.subplot(7,7,num+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    path = dataset_directory +'/'+ i #"/meta/{}.png".format(i)
    img = plt.imread(path)
    plt.imshow(img)
    label =str( Meta_images[Meta_images['Path']==i]['ClassId'].values[0])
    plt.xlabel(label)
    num+=1

# Show images of a traffic sign and it's labels 

plt.figure(figsize=(10, 10))
num = 0
for i in (Train_images.sample(frac = 1))['Path']:
    plt.subplot(7,7,num+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    path = dataset_directory +'/'+ i #"/meta/{}.png".format(i)
    img = plt.imread(path)
    plt.imshow(img)
    label = Train_images[Train_images['Path']==i]['ClassId'].values[0]
    plt.xlabel(label)
    num+=1
    if num == 43 :
      break

"""# Preprocessing Data

##List to Numpy Array
"""

# install MPI for Python
!pip install mpi4py

# Commented out IPython magic to ensure Python compatibility.
# # In case you work on multi-core machine , this code will be useful to minimize execution time
# # MPI for Python provides Python bindings for the Message Passing Interface (MPI) 
# # standard, allowing Python applications to exploit multiple processors on workstations, clusters and supercomputers.
# # This package builds on the MPI specification and provides an object oriented 
# # interface resembling the MPI-2 C++ bindings. It supports point-to-point (sends, receives)
# # and collective (broadcasts, scatters, gathers) communication of any picklable Python object, 
# # as well as efficient communication of Python objects exposing the Python buffer interface (e.g. NumPy arrays and builtin bytes/array/memoryview objects).
# 
# 
# # converte images to numpy array and collect all of them in list 
#  
# %%time 
# from mpi4py import MPI
# import pandas as pd 
# import csv
# import numpy as np
# import cv2
# 
# def split(a, n):
#     k, m = divmod(len(a), n)
#     return list(a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))
# 
# comm = MPI.COMM_WORLD
# size = comm.Get_size()
# rank = comm.Get_rank()
# 
# 
# df = []
# if rank == 0:
#   MPI_data = []
#   MPI_labels_new = []
#   Train_images = pd.read_csv('/content/Train.csv')
#   paths = split(np.array(Train_images['Path']), size)
# 
# 
# Sub_data = []
# recv_paths = []
# Sub_labels = []
# 
# 
# recv_data = comm.bcast(Train_images, root=0)
# recv_paths = comm.scatter(paths, root=0)
# 
# for path in recv_paths:  
#     try:
#         image = cv2.imread(dataset_directory +'/'+ path)
#         image_f = Image.fromarray(image, 'RGB')
#         size_image = image_f.resize((30, 30))
#         Sub_data.append(np.array(size_image))
#         label = recv_data[recv_data['Path']==path]['ClassId'].values[0]
#         Sub_labels.append(label)
#         
#         
# 
#         
#     except:
#         print("Error in " + path)
#         break
# 
# MPI_data = comm.gather(Sub_data, root=0)
# MPI_labels = comm.gather(Sub_labels, root=0)
# 
# if rank ==0 :
#   sign = 0
# 
# 
# 
#

print(((np.array(MPI_data)).reshape(39209, 30, 30, 3)).shape)
print(((np.array(MPI_labels)).reshape(-1,)).shape)

# convert lists to numpy array and reshaping it to be suitabl to split and the CNN model
MPI_data = (np.array(MPI_data)).reshape(39209, 30, 30, 3)
MPI_labels = (np.array(MPI_labels)).reshape(-1,)
MPI_data = np.array(MPI_data)
MPI_labels = np.array(MPI_labels)

# In case you run the code on google colab /jupyter notebook ,use and run this cell
# The same taske done here with this code.Approximately, with total exceution time = 2.4 min
# [It could be (min/max)imum  on different machine, that depends on the computationl power] 

# %%time 
# import numpy as np
# import matplotlib.image as mpimg

# data = []
# labels = []

# for path in Train_images['Path']:
#         try:
#             image = mpimg.imread(dataset_directory + '/' + path)
#             image_f = Image.fromarray(image, 'RGB')
#             size_image = image_f.resize((30, 30))
#             data.append(np.array(size_image))
#             label = Train_images[Train_images['Path']==path]['ClassId'].values[0]
#             labels.append(label)
#         except:
#             print("Error in " + img)
#             break

# data = np.array(data)
# labels = np.array(labels)

# sign = 1  

#2.4s

"""##Shuffling"""

# Shuffling the data of images and it's label to avoid overfitting 
# Shuffling is done in the same order in which images will still match the right label  
# { Example below on the functionality how does it work}
if sign == 0 :
  used_data = MPI_data
  used_labels = MPI_labels
  shuffle_indexes = np.arange(used_data.shape[0])
  np.random.shuffle(shuffle_indexes)
  used_data = used_data[shuffle_indexes]
  used_labels = used_labels[shuffle_indexes]


else :
  used_data = data
  used_labels = labels
  shuffle_indexes = np.arange(used_data.shape[0])
  np.random.shuffle(shuffle_indexes)
  used_data = used_data[shuffle_indexes]
  used_labels = used_labels[shuffle_indexes]

# Example-1
import numpy as np
a = np.arange(100,200)
shuffle_indexes = np.arange(a.shape[0])
np.random.shuffle(shuffle_indexes)
b = a[shuffle_indexes]
print("Shuffling order =>\n",shuffle_indexes)
print('-'*80)
print("array a before suffling =>\n",a)
print('-'*80)
print( "array a after shuffling =>\n",b)

"""##Splittng, Scaling and Encoding  """

#Splitting the data to go throw next step , Building the CNN model
x_train, x_val, y_train, y_val = train_test_split(used_data,used_labels, test_size=0.3, random_state=42, shuffle=True)

#scaling the data
x_train = x_train/255 
x_val = x_val/255

print("X_train.shape", x_train.shape)
print("X_valid.shape", x_val.shape)
print("y_train.shape", y_train.shape)
print("y_valid.shape", y_val.shape)

#encode the labels to fit with the model
y_train = keras.utils.to_categorical(y_train, 43)
y_val = keras.utils.to_categorical(y_val, 43)

print(y_train.shape)
print(y_val.shape)

model0 = tf.keras.models.Sequential([
     keras.layers.Flatten(),
     keras.layers.Dense(512,activation='relu'),
     keras.layers.Normalization(),
     keras.layers.Dense(256,activation='relu'),
     keras.layers.Dropout(0.5),
     keras.layers.Dense(128,activation='relu'),
     keras.layers.Dense(64,activation='relu'),
     keras.layers.Dropout(0.5),
     keras.layers.Dense(43, activation='softmax')

])

model0.compile(loss='categorical_crossentropy',
               optimizer="SGD",
                metrics=['accuracy'])
history = model0.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))

model1 = tf.keras.models.Sequential([
     keras.layers.Flatten(),
     keras.layers.Dense(512,activation='relu'),
     keras.layers.Normalization(),
     keras.layers.Dense(256,activation='relu'),
     keras.layers.Dropout(0.5),
     keras.layers.Dense(128,activation='relu'),
     keras.layers.Dense(64,activation='relu'),
     keras.layers.Dropout(0.5),
     keras.layers.Dense(43, activation='softmax')

])

model1.compile(loss='categorical_crossentropy',
               optimizer="Adam",
                metrics=['accuracy'])
history = model1.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))

from tensorflow.keras.layers import LeakyReLU

model2 = tf.keras.models.Sequential([
     keras.layers.Flatten(),
     keras.layers.Dense(512,activation=LeakyReLU()),
     keras.layers.Normalization(),
     keras.layers.Dense(256,activation=LeakyReLU()),
     keras.layers.Dropout(0.5),
     keras.layers.Dense(128,activation=LeakyReLU()),
     keras.layers.Dense(64,activation=LeakyReLU()),
     keras.layers.Dropout(0.5),
     keras.layers.Dense(43, activation='softmax')

])


model2.compile(loss='categorical_crossentropy',
               optimizer="Adam",
                metrics=['accuracy'])
history = model2.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))

"""#Pretrained Model => ResNet50"""

used_labels
used_data

model_A = keras.applications.resnet50.ResNet50(weights='imagenet')
resized = tf.image.resize( used_data ,[224 , 224])
new_data = keras.applications.resnet50.preprocess_input(resized)
preds = model_A.predict(new_data)

"""#Build CNN Model """

earlyStop = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    min_delta=0.003,
    patience=4,
    verbose=0,
    mode='max',
    restore_best_weights=True,

)
model = tf.keras.models.Sequential([
 tf.keras.layers.Conv2D(filters=64, kernel_size=(4,4), activation='relu', input_shape=(30,30,3)),
 tf.keras.layers.MaxPool2D(pool_size=(2, 2)),
 tf.keras.layers.Conv2D(filters=64, kernel_size=(4,4), activation='relu',padding="same"),
 tf.keras.layers.Dropout(0.5),
 tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), activation='relu',padding="valid"),
 tf.keras.layers.MaxPool2D(),
 tf.keras.layers.Conv2D(filters=16, kernel_size=(4,4), activation='relu',padding="same"),
 tf.keras.layers.Flatten(),
 tf.keras.layers.Normalization(),
 tf.keras.layers.Dense(1024, activation='relu'),
 tf.keras.layers.Dropout(0.5),
 tf.keras.layers.Dense(43, activation='softmax')])

model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

"""##Model Results"""

history = model.fit(x_train, y_train, batch_size=32, epochs=20, validation_data=(x_val, y_val), callbacks=[earlyStop])

#From some notebook on the competition
classes = { 0:'Speed limit (20km/h)',
            1:'Speed limit (30km/h)', 
            2:'Speed limit (50km/h)', 
            3:'Speed limit (60km/h)', 
            4:'Speed limit (70km/h)', 
            5:'Speed limit (80km/h)', 
            6:'End of speed limit (80km/h)', 
            7:'Speed limit (100km/h)', 
            8:'Speed limit (120km/h)', 
            9:'No passing', 
            10:'No passing veh over 3.5 tons', 
            11:'Right-of-way at intersection', 
            12:'Priority road', 
            13:'Yield', 
            14:'Stop', 
            15:'No vehicles', 
            16:'Veh > 3.5 tons prohibited', 
            17:'No entry', 
            18:'General caution', 
            19:'Dangerous curve left', 
            20:'Dangerous curve right', 
            21:'Double curve', 
            22:'Bumpy road', 
            23:'Slippery road', 
            24:'Road narrows on the right', 
            25:'Road work', 
            26:'Traffic signals', 
            27:'Pedestrians', 
            28:'Children crossing', 
            29:'Bicycles crossing', 
            30:'Beware of ice/snow',
            31:'Wild animals crossing', 
            32:'End speed + passing limits', 
            33:'Turn right ahead', 
            34:'Turn left ahead', 
            35:'Ahead only', 
            36:'Go straight or right', 
            37:'Go straight or left', 
            38:'Keep right', 
            39:'Keep left', 
            40:'Roundabout mandatory', 
            41:'End of no passing', 
            42:'End no passing veh > 3.5 tons' }

def getX_test(Image_Path):
  image_test = cv2.imread( Image_Path)
  image_f_test = Image.fromarray(image_test, 'RGB')
  size_image_test = image_f_test.resize((30, 30))
  image_data_test = np.array(size_image_test)

  x_test_show = np.array(image_data_test)
  x_test = x_test_show/255
  x_test=x_test.reshape(-1,30,30,3)
  
  return x_test, x_test_show


def predictClass(model, Image_Path : str, dict_of_signs_label):
  x_test,x_test_show = getX_test(Image_Path)

  prediction = model.predict(x_test)
  index=np.argmax(prediction , axis=1)
  plt.imshow(x_test_show)
  print('Class :',index ,' ==> {',dict_of_signs_label[index[0]],'}')

predictClass(model,'/content/Test/00700.png',classes)

pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

plt.plot(history.history['accuracy'], 
         label='Training accuracy')
plt.plot(history.history['val_accuracy'], 
         label='Validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], 
         label='Training loss')
plt.plot(history.history['val_loss'], 
         label='Validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""## Saving Best Model """

import pickle
pickle.dump(model,open('/content/BestModel.pkl','wb') )
saved_model = pickle.load(open('/content/BestModel.pkl','rb'))

predictClass(saved_model,"/content/Test/00167.png",classes)